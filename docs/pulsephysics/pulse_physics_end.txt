ljunggren audio's 'pulse physics' is based around a synthesis technique largely unexplored in the majority of commercial hardware synthesisers so far existing - pulsar synthesis:

------

in the audio world, [filters] alter the harmonic content of a signal - i.e. they can attenuate, pass or boost a band or number of frequency bands. as such this means they can remove, allow to pass unchanged or even add harmonic content to an audio signal. complex filters can even do multiple actions from those mentioned above, passing some portions of a signal unchanged, attenuating or boosting others and even introducing new content entirely in yet other bands.

------
boosting of a signal or introducing actual new harmonic content not preiously present, when considering audio filters, is most commonly achieved via the use of feedack loops.

many will be familiar with audio [feedback] as the brain curdling squeal from moving a microphone too close to a speaker or turning the mic gain/speaker volume too high. all that is occurring here is the mic is picking up the output of the speakers, then the system amplifies it before outputting from the speakers, and then it is picked up, this time at a greater volume, by the microphone only to be sent around the amplification loop generated yet again - until eventually it becomes intolerable or the equipiment is damaged.

------
when employed in an audio filter, a feedback loop does the same thing as mentioned above, re-amplifying a signal by passing the output repeatedly back to the input, but since the signal is passing through a filter, only certain frequencies are being fed back so either a limited range of bands are boosted/introduced into the output signal, or the fed back sounds are repeatedly processed by the filter so having more and more of a given frequency band altered/created effectively as [new harmonic content] since the amplified signals now shifted by the filter passband are no longer at the frequency of the original input signal.

------
[self-oscillation] in an audio filter occurs when a resonant filter [i.e. on  with a feedback loop present in the system] has the gain of the feedbck loop sufficiently high that it doesn't die away with no input signal present and will inherently generate an audio signal of its own simply via the feedback loop i.e. it resonates or oscillates. in other words the simple presence of an amplifier with a feedback loop system is sufficient in itself to act as an audio oscillator and hence generate a signal.
such a filter system is said to be setup to be self-oscillting. 

------
if a resonant audio filter is setup to be just on the cusp of self-oscillation, but not actually generating a signal and then an excitatory signal is fed into the system it will be enough to push it out of it's current equilibrium state and over the edge into self-oscillation. if this excitatory signal is a brief burst of noise or other short spike, when the self-oscillation state ceases, the oscillations will gradually die back down, returning to the rest state of non-oscillation, i.e. silence, gradually as the feedback loop cycles through repeats of less and less signal being passed through each repeat until it nothing is left to be passed appreciably. this gives an effect of a 'pluck' of signal as there is an initial peak of sound followed by a dying away - known in some synthesis circles as 'pinging' a filter.

filter systems such as this 'pluckable' filter are known as [_resonators_].

------
related to filter based resonators such as the above are [Karplus-Strong resonators]. rather than using filters, this method uses a _very short_, audio-rate delay with a lowpass filter in the feedback loop to create plucked string sounds when excited with a noise burst. When fed constant noise as an excitatory source, it produces results somewhat akin to a bowed string sound.

*note*: technically, on paper/mathematically audio-rate delays and audio fiters are extremely similar or even identical cases of the same applied maths, so it's no surprise that when setup similarly - i.e. filter/delay with a feedback loop present the result is a resonator system. although the specifics of a karplus-strong resonator using a delay over an audio filter matters more when it comes to applications such as real world analogue electronics and when modelling the imperfections of more real world systems rather than mathematical setups, it does gives some hints of possible ways to experiment and vary the setup, for example such as swapping the lowpass filter in the k-s feedback loop for another type of filter, or introducing phase shifts in the signal at various points such as when the signal is returned to the system from the feedback loop.

------
as mentioned above, a number of variations of the k-s resonator system possible inspire yet further new resonator setups. one possible is employing multiple, parallel filters in a feedback loop - specifically a whole filterbank of bandpass filters, preferrably a minimum of 8, which results in a specific type of resonator named a 'modal' resonator.

in general, modal synthesis is a form of physical modelling synthesis - i.e. attempts to recreate sounds generated by physical objects via modelling their behaviour.

consider when, for example, a physical object such as a wooden bar, metal tube or plastic sheet, etc. is struck - as vibrations travel up and dow the axis along which sound generating oscillations occur, 'modes' are generated - a mode is where natural resonances meet - a length along which groups of oscillating sections are created by the division of the resonating axis into two or more mulitple, equal oscillating sections. the specifics of these modes determine the nature of the resulting sound.

recreating modes mathematically and using that math to calculate the output parameters for a synthesiser, e.g. the number of modes in a real life situation dictating the number of filters in a filterbank employed in the feedback loop of a modal resonator, we can recreate [synthesise] the sound of such an object - in this instance we have created a 'modal resonator'.

if we then trigger this modal resonator system to sound using a 'complex exciter source' - the mathematical equivalent of using a mallet or drumstick or padded beater to strike a gong or snare drum or whatever, [i.e. we pluck the resonant filters of the system as done when 'pinging' the simpler resonator systems dicussed earlier] we have now employed modal sythesis.   

------
mi 'elements':

the now defunct 'mutable istruments' synthesiser company produced a hardware based code synthesis voice employing modal synthesis using the concept discussed above as its core.

it employed 64 state variable filters in its filterbank;
it had control over general resonator parameters such as:
- 'geometry', emulating a variety of structures such as plates, strings, bowls and tubes;
- 'brightness', specifying the character of the resonator material, from wood to glass to nylon to steel;
- 'damping', simulating a wet material or the muting of the vibrations;
- 'position', specifying the distance of the exciter from the resonator output pickup;
there was a complex exciter section, consisting of three mixable sources, bowed/struck/blown.
the bowed and blown exciter sources had a contour control to shape the input signal since they were not just short sharp impulses.
there were granular based noise generators for bowed and blowing sounds to produce continuous/extended signals as well as the initial trigger impulse - these had a degree of tonal variation based variously on either a two pole filter or wavetable type scanning between sounds and a percussive gernerator based on an interpolated scan between sources such as plectrums/damped mallets/bouncy particles/etc. with pitch control and a 2 pole filter.
there was ability to process external input sounds, both pre-envolope and direct to resonator.
all of these various sound sources and exciters could be combined and controlled in various ways through mixers and volume controls.
there was frequency control over the whole output signal to make the thing tonal in nature if chosen.
the end result was output as stereo audio.

the open source code for this synth voice is available via github at present date.

------
------

[this next section is morre for elevnce of computational efficiency considerations and the reasoning behind the use/development of the actual final desired synthesis technique, not because the actual techniques/computations/mathematics so relevant/contained in any way here are particularly important overall.]

so far all the above discussed synthesis concepts have looked at the functioning of these synthesis techniques and considered backgrounds based on real world physics or electronic behaviours of systems, and hence although often employed in the world of synthesis, both analogue and software, are usually when recreated mathematically and coded to become software synths totally re-evaluated mathematically to be much more efficient computationally, as well as, where possible avoiding issues such as aliasing of the output audio waves or sudden/excessive runaway spikes in data volume required to be computed when such features as feedback loops are being processed. since software synthesis suffers from these issues specifically when analogue electronic synthesis doesnt, since analogue synthesis simply will end up with a screaming howl or self oscillation or phase cancelled silence etc. when things go wrong/out of control, a number of novel techniques have been evolved specifcally as software synths have been developed to deal with such issues:

consider at any given time only certain calculation results from the usually surprisingly simpel and elegant core synthesis equation will be appreciably audible. the resulting procsses of these very complex and long winded calculations that arise due to the nature of stuff like reiterations through loop features, polyphonic aspects, branching trees of cascading results from a simple initial term producing endless resultant sub-calculations as more and more harmonics/subfrequencies/modes/resonances spin off etc. and other such challenges involved in dealing with the emulation of real world/complex sounds that have more than just one oscillating source affected by a single filter or contoured amplifier etc., even though stemming from a basic alegbraic set of rules, will often require prioritisation or even not being calulated at all, based on rules dictated by limiting math which delineates what amplitudes and frequencies of an individual 'note' will be necessary to compute, what 'notes' to cut off given a particular duration limit, and therefore as a whole, what parts of the overall more complex but combined equation being calulated at any given time need be carried out to give an appreciable output sound that whilst not differing from the sound that might be output if every single part of the original calculation as a whole was computed. the end result is effectively identical to the ear, i.e. although the synthesis results are dictated by a relatively elegant and simple equation such as 'output = frequency x amplitude x decay time', in reality there are intructions saying 'frequency = values that lie within the range resulting from outputting only 22hz - 22khz. ignore anything outside this range.'

another technique is to employ a band limited impulse train [BLIT] - by creating a stream of sinc() functions [sinc x = (sin x)/x] whih are low pass filtered, and then integrating the stream over time. [possibly of interst/relevnce to coders here, the normalized sinc function is the fourier transform of the rectangular function with no scaling.] it is worth noting, the impulse response of an ideal lowpass filter is band-limited by its very nature, so requires no further complexity to the mathematical equations to delineate any boundaries to avoid unecessary calculations being computed. crucially, integrating the impulse train in this situation, depending on polarity of the input, will result in either, a sawtooth wave from a unipolar, a square wave from a bipolar or a triangle wave from doubly integrating a bipolar BLIT. there will be resulting complexitiees like rounded discontinuities, ringing on the  band edges of the waveforms and aliasing to some degree, and since somewhat computationally expensive for real time use and hence often stored as a reeult in a table and period of the desired waveform may not be an integer multiple of the sample rate, the table has to be interpolated when the algorithm is run. note, since integration is a filter with s-plane transfer function of H(s)=1/s
 and is LTI (Linear, Time-Invariant), integrating the BLITs will introduce no new frequency components, therefore since BLITs are bandlimited, so are the other waveforms that are derived from filtering the BLITs.

since the BLIT technique, although greatly advantageous mathematically, and hence computationally, compared to the straight up mathematical emulation of physical/analogue electronic systems as far as synthesis goes, is still not entirely well suited for real time synthesis as far as siftware options go, and so it was further evolved into:

Band-limited Step Function [BLEP] synthesis. as metioned in the above desciption of BLIT synthesis, the impulse response of an ideal low pass filter when integrated as a bipolar form transforms it into a step function [see as square wave when processed as a train]. if the ideal unit step function is then subtracted from the calculated step function we form a residual based on the specific time/frequency parameters as dictated. this residual is stored in a table, then merged with a ‘naïve’ (i.e. aliased)  waveshape generated mathematically to form the final BLEP output, offsetting the samples around the discontinuities by shifting them up or down to produce a smoothed waveform. BLEP synthesis is less computationally heavy than BLIT, produces smoother waveforms and can take advantage of a number of options such as windowed sinc() functions, two/four/eight point discontinuity correction and linear interpolation of the BLEP table etc. to further improve computational efficiency/sound quality.

it can be seen therefore that when using purely software considerations to take advantage of mathematical functions, as well as simple limits upon emulation of physical/analogue electronic systems there are ways to us software synthesis to do some fairly complex and even non-exisitent in physical/analogue electronic synthesis systems, even with very good sound quality and efficient computational options to achieve decent results.

------

the reason for metioning the specific software synthesis computational methods above and also for detailing the aspects highlighted in prticular will become apparent below - they deal with synthesising various forms of relatively complex analogue and natural, physical model derived synth waveforms - they are based on the integral (over t) of impulse trains, but with the impulse δ(t−tn) replaced by a sinc(t−tn) function. 

back in the 1950s instruments such as the Ondioline and Honer Elektroniu were developed, based around the principle of filtered pulse trains. 


pulsar synthesis.

built using 2 more basic synthesis principles:
- impulse synthesis;
- amplitude modulation synthesis;

pulsar synthesis involves using an impulse as the modulator for a carrier signal in an an amplitude modulation synthesis setup, but with some specific limitations:
- the two signals, carrier and modulator, should be phase locked (not hard synced) such that they start in phase;
- they should be tonal rather than atonal (so an actual musical wave sound is produced instead of noise);
- the modulation signal should have an obvious period of silence within the total period of the waveform which can be varied.

i.e. there should be an input tonal carrier wave passing though an amplifier which is contoured by a tonal impulse, the two waveforms [sharing a trigger such that the initial phase of the modulator is locked to the carrier(? - my words, and hence slight guess...)], with a period of silence being present at some point in the modulation signal, finally the output signal can be fed onward for further processing as desired.

*wofl note: multiple modulation signals with lower frequency than the period may be employed for a unique smearing effect, and they can be of differing shape.
**wofl note: unipolar waveforms for the impulse modulator waveform are generally advantageous, but it is generally important to ensure that the unipolar nature is generated only by halfwave rectification, _not_ fullwave.

------
'pinging' resonant audio filters acting as resonators in a system, as discussed above is called 'impulse synthesis'.

since pulsar synthesis employs impulse synthesis as part of its' core principle, and we have seen that impulse synthesis can be incresed in complexity by developing the resonating body to something more advanced employing a whole bank of filters resulting in modal synthesis, this begs the question, what if we replace the impulse generator providing the modulation source in a pulsar synthesiser with a modal synthesis setup instead of just an impulse synthesis setup?

we get the concept for the ljunggren audio 'pulse physics' - a pulsar synthesis voice employing modal synthesis as the impulse source modulation waveform.

------
the ljunggren audio 'pulse physics' synthesiser voice will employ the mutable instruments open source 64 filter filterbank modal synth engine as the core for an impulse modulation waveform source to act upon a variety of basic waveforms provided as carrier waves to produce a complex pulsar synthesis type synth voice capable of immensely rich, varied and creatively controllable sounds.

------

from my very brief, very uneducated and very optimistic, but possibly deeply erroneus understanding of some of how the code for ni elements operates: 
- the trigger exciter waveform sources are simply rawdata .wav files called from a bank then processed by code for contouring/filtering/amplifying/etc.
- i'm hoping, but of absolutely no clue if, the same applies to the sound sources for the continuous exciter waveforms - the mention of 'granular' in the spec description and use of other terms such as 'interpolated' and 'filter' makes it possible there is a more synthesis based generation option employed.
- the basic operands for the whole synth voice in general are coded such that, for example a contour envelope is defined by a number of parameter limits between which maximum and minimum values lie - time/peak amplitude/etc. and these operands are then chained together such that a contour opens/closes an amplifier which feeds into a filter etc. - i.e. the whole code follows a logical architecture graspable according to standard synthesis theory.
- most parameters used, both in a real world e.g. '44.1khz/16bit' sense or as code in relevant programming language terms, are, generally specified somewhere, and therefore not only relatively easy to refrence, but also to some degree edit and vary as desired.
- much of the basic math the actual synnthesis concept is based aroud, at core, is available in papers like curtis rhodes'
original writings on pulsar synthesis and hence can be used both as reference and room for further expansion. likewise, not only is the existing ni elements code/math extremely well documented, but also even if no reference is to be found, 90% of the math is basic geometric algebra, and that which isnt is for the most part calculus 101 - nothing too far beyond the capbility of those with some experience of audio level math. 

***fundamentally all possible changes to the nature of the way the thing works that i desire and have come up with comceptually rely to a degree on the exising code layout being relatively modular in nature - fortunately, the degree to which i have banked on this is kinda at the level the actual math and logic around which the code is based around is known to be by its very essence 'modular'. i.e. i'm not just pulling shit outta thin air.***

------
[desired but not yet investigated]

conceptually i would like to steer the sound of this project a little further away from the somewhat 'traditional' nature of the ni elements wind/mallet/string based exciter with bowl/tube/plate resonator featureset and control over damping/distance/brightness/etc. and instead lean towards a much more 'synthetic'/experimental kinda sound.

what this will actually result in as far as direct real world results of swapping a sample of a violin bow as an exiter or altering the parameters of the geometry of the resonator to things less sat squarely in the realm of real acoustic instruments, i honestly dont fully know.
what i do know is the biggest risk likely to present itself is destroying the finely tweaked and tuned setup of the elements math limits currently coded in place such that most likely no sound is produced, or maybe resulting in something just plain dissapointing/utterly awful. on the other hand, with careful experimentation, gradually pushing limits of parameter boundaries, and keeping changes on the simple side at first before getting too creative, it should be possible to create something really quite novel and special.

i should stress here, i have the advantage of 30 yrs of experience as far as experimentation with synthesis on my side, so i know very well the potential pitfalls of just randomly making changes and hoping good sounds will result - likewise, i know quite well how best to proceed to get decent results and also have a deep understanding of what's actully going on under the hood and hence what to expect from any given change.

[so, my current plans for changes incude, if feasible:]

- present probably just selection of simple saw/square/sine/maybe a few other options as carrier wave.
- although the bowed/struck/blown sections are no longer present as broken down separate parts, instead, there's a selectable variety of [posssiby interpolated between?] sound sources as triggers for the modal resonator. [contour control may not be relevant here if simplcity is kept under wraps by sticking to a range of fairly basic fast attack sourcewave options to choose from as triggers].
- instead of granular, tone controlled/selectable percussive 'strike' with pitch and filter option generators for continuous source options, instead have [?] some sort of interesting[/not so] sound sources as befits best sound after testing. [depends on whether a choice of intersting electronic synthy sources offers a more creative/less 'traditional' synth sound or just results in a mess].
- probably not have the external input option as present in the ni elements, although given that the sound options would already be limited if the bowed/struck/blown sections and continuous source options are removed/reduced, then maybe this would prove essential as something to leave in - to the point that having it useable as a sound processor as much as generator might even be key to the whole success of the thing.

------
real analogue circuitry is cool right now - 'vintage' in the industry in general is in - i.e. super popular, and as with any other 'fashion over actual engineering' based consideration, it desnt really matter quite how much snakeoil the actual reality of it's implementation is, as long as its present in a real enough sense to be able to legally claim it's there, any actual analogue electronic circuit that can be tacked on, preferrably for the lowest cost/space/time taken to achieve will technically do, whilst integrity as far as caring about making a decent sounding, relevant pieces of design gets set aside.

since at any point within the signal flow, interrupting it to break-out and then reinsert a signal, from digital to analogue and back to digital, would require dac/adc implementation and a whole buncha other excessively unnecessary circuitry/design/engineering problems, it seems, realistically the only choice is to place the analogue circuitry element at the final output of the synth voice where not only is there no need for re-insertion of the signal to the signalflow, but also a dac is already present as part of existing design setup to some degree.
experience and instinct suggests the two best options are either a filter or vca of some sort.

::a vca has the advantages that it can offer an honest, true improvement in sound and allows option of utility purposes - contouring of the end sound ouput/application of AM/provide an exit point for eurorack compatibility/etc., but a vca isnt the simplest circuit to build as far as parts/budget/complexity, and also isnt the most exciting opportunity as far as marketing goes, which fundamentally is the point of the exercise;
::a filter has the advantges that it has a very appreciable effect on the final sound, can be extremely simple to implement and is a marketing dream as far as people just kinda get em on some level - they have an idea of how they sound when you 'twist-the-knob'. nailing a filter on the end does have the disadvantage you are now restricted to a paraphonic mono output or have to delve into same horrendous complexities/expense to solve stereo and balance issues.

further to the ends of adding an analogue circuitry both marketing and actual benefits to sound quality suggest a discrete component based circuit, which although more customisable as far as tuning to the specifics of this is a bonus, it makes it less efficient spacewise/more complex/more expensive/a particular problem as far as vcas are concerned because it excludes the use of ic based vca solutions - the natural way out of most difficulties stemming from the commmon requirement for matched transistor pairs or hard to source transistors required for the core gain control element of even a discrete vca.

all this strongly points towards adding a discrete analogue circuit based mono paraphonic filter setup at the end of the signal chain of the synth voice.

------
- since the effects section present in the ni elements open source code only affects the modal synth part of the synth acting as the impulse generator, and not the overall pulsar voice as a whole;
- and no matter how good an effects section is, it will never compete with external effects, whether provided by the user of the synth via other hardware effects units/processing through items like modular synths/further online and offline processing with software in computer daws etc. downstream;
- furthermore, in the case of elements, certain effects options were very poorly implented in the original circumstance anyways - i.e. the 'reverb' being one control, amount present, adjustable by turning the same knob that did the output volume from past-halfway to max;

- similarly, the entire eurorack compatible 'cv' control system for applying control voltages allowing automation/processing and connection of the elements voice to other eurorack gear can be completely stripped out - i pray  with all my heart this was achieved via a relatively mechanically based engineering solution and so there isnt a fuckton of code to allow for an adc in place converting cv signals present on every single rotary encoder from analogue voltage to digital controller values...
- i'm not sure a full coarse/fine tuning over modal impulse modulator source is necessary - and especially not fm option. fm input/control over carrier wave on the other hand might be desirable.

------
to be researched:
- do people want a 'stand-alone' type single trigger button? would be useful whilst programming just to check what current sound is like.
- do people want built in keyboard or equiv control source?
- what else?

------
i do know:
- synth voice pitch is defined mostly by the carrier wave frequency.
- modulator voice pitch, i.e. modal synth pitch is defined by waveform frequency not resonator length - i.e. not by filter frequency.
- timbre of the overall synth voice is strongly defined by the relationship between the phase and pulsewidth/waveshape of the carrier and modulator.
- timbre of the modulator voice is defined by the pulsewidth/waveshape of the modulator waveform.
- modulator formant is defined by the cutoff frequency of the modal synth voice filters.
- amount [read 'level'] of an individual voice element in the modal synth modulator impulse is dictated by the resonance of that filter element.
- overall length of an impulse sound from a modal synth voice sound is defined by both the resonance and the frequency of the longest [i.e. highest frequency vs. highest resonance] element of the modal synth voice, for a given pitch.
- modal synth impulses created with a higher attack and greater volume overall exciter strike will be louder and last longer.
- frequency balance of the filter bank makeup in the modal synth will affect the brightness of sound and to some degree the length of the impulse/volume it presents at but with careful control over individual elements a great degree of variety can be achieved whilst retaining consistent duration/volume of final result.

since a great degree of complexity in control is available over formant/timbre/level balance/individual level/element duration etc., by mixing different lengths and levels of various timbres/formants, levels and durations of the various chosen elements resulting in these, an incredible variety of resulting sounds can be achieved even ignoring choice of actual exciter/waveform/variety in such pre-/post- sound options.

it is likely the filters in the elements filterbank not only have control over cutoff and q, but also actual level so allowing separate control of mixing of any element as well as the more frequency/time dependent use of q to control level.


------
items of deep importance that might be cruicial to things as they are/likewise might lead to even more fantastic things than current concept...

- partly because i'm not certain, but, does it matter whether a modal synth is based on a v short delay vs. a resonant filter? according to my understandinng no because a) mathematically the two are basically the same and i think stuff like elements is implemented with a filterbank of independent 'single' filters in parallel - i.e. 64 individual statevariable filters operating in parallel, so behaviour with respect to any other element in the system is moot.
- if so, and excitingly so as a prospect, what happens if in some way a multiple-element filter/multiple filters in series per-element or especially something like a comb filter is used per element?
- suddenly controls over parameters such as 'spread' as well as individual bandwidths or various cutoff frequencies/q/etc. become interestingly possible.

------

